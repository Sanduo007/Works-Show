# Works Show
The author of this repository is a master candidate in Navigation, Guidance and Control in GNSS Research Center at Wuhan University, supervised by Prof. Xiaoji Niu. 
<br/>
This repository is about part of his research works in his master years as well as a complementary file for his Ph.D. application. If you are interested in anything of or have any question about it, please feel free to contact him <ybwu@whu.edu.cn>. 
<br/>


## 1. Wheel-INS 
 A wheel robot controlled by a smartphone through WiFi was designed and built for this research (*Wheel Mounted MEMS IMU Based Integrated Navigation System for Ground Vehicles*).

<img src="https://github.com/Sanduo007/Works-Show/blob/master/img/WMRobot.png" width = 50% height = 50% div align=center />

    
<br/>

Positioning results on the horizontal plane and the pose error along with time for **small-scale loop trajectory** in one test are shown as follow. 

<img src="https://github.com/Sanduo007/Works-Show/blob/master/img/0701DR.png" width = 30% height = 30% div align=left />
<img src="https://github.com/Sanduo007/Works-Show/blob/master/img/0701poserr.png" width = 30% height = 30% div align=center />              
<img src="https://github.com/Sanduo007/Works-Show/blob/master/img/0701headingerr.png" width = 60% height = 60% />

Positioning results on the horizontal plane and the pose error along with time for **large-scale polyline trajectory** in one test are shown as follow. 
<br/>

<img src="https://github.com/Sanduo007/Works-Show/blob/master/img/0720DR.png" width = 30% height = 30% div align=left />
<img src="https://github.com/Sanduo007/Works-Show/blob/master/img/0720poserr.png" width = 30% height = 30% div align=center />
<img src="https://github.com/Sanduo007/Works-Show/blob/master/img/0720headingerr.png" width = 60% height = 60% />

## 2 GNSSRTK/MEMS IMU/Odometer integrated navigation
 This figure shows the positioning results of the proposed system comparing with GNSS/INS and GNSS/INS/NHC integrated navigation. The experiment was conducted at Wuhan City. The ground truth was generated by high-end GNSS/INS integrated system. The enlarged part in the figure is the end of the tunnel. It can be observed that the vehicle velocity along with non-holonomic constraints (NHCs) contributed much to **suppress the position drift** in the GNSS-denied situations.
<br/>

<img src="https://github.com/Sanduo007/Works-Show/blob/master/img/google.png" width = 50% height = 50% />

<br/>

## 3. Pose estimation for UAV landing 

The idea of this work is to fuse the UAV pose retrieved by landing marker detection with the MEMS IMU data to achieve precision, continuous and high rate positioning for UAV landing. The related paper was published in [*Sensors*](https://doi.org/10.3390/s19245428) (open access).


## 4. Express UAV 

The drone was designed for expressing delivery. It can automatically takeoff and fly to the destination by GNSS/INS integrated positioning with collision avoidance by ultrasonic sensors, and then land on the pad by detecting the previously placed marker using an onboard camera. The video shows the key technologies. https://youtu.be/mQtUiN1oKXY

<img src="https://github.com/Sanduo007/Works-Show/blob/master/img/ExpressUAV.png" width = 50% height = 50% div align=center />

<br/>

## 5. Some competition awards
5.1 **First prize (1.97%)** and  **"Best Paper Award"** in the 14th China Graduate Electronic Design Competition (*based on the work of Collaborative Precision Positioning and Navigation System of Robot*) (Aug.2019)

In this work, we built an autonomous-driving robot. The self-developed GNSS/MEMS IMU/Odometer module was used to obtain real-time localization results. Lidar was used to detect obstacles. Camera was utilized to detect the lane line and marker to aid INS. The artificial potential field method was introduced for path planning.

Here is a video illustrating our work. https://youtu.be/3vmfz4duIoE

<img src="https://github.com/Sanduo007/Works-Show/blob/master/img/EDcomment.png" width = 40% height = 40% div align=left />
<img src="https://github.com/Sanduo007/Works-Show/blob/master/img/ElectronicDesignRobot.png" width = 40% height = 40% div align=center />

<br/>

5.2 **Third prize** in the 6th China Graduate Contest on Smart-city Technology and Creative Design (*based on the work of express UAV*) (Aug.2019)

<img src="https://github.com/Sanduo007/Works-Show/blob/master/img/SmartCity.png" width = 40% height = 40% />

<br/>

## 6. SLAM Learning

***Introduction of EPnP*** (4 pages) in the repo is an introduction of the EPnP (proposed by Computer Vision Laboratory at EPFL) algorithm with detailed equations derivation.
<br/>

***Equations Derivation of VINS-Mono*** (26 pages) is an analysis of the [VINS-Mono](https://github.com/HKUST-Aerial-Robotics/VINS-Mono) (a monocular-VIO system proposed by Aerial Robotics Group at HKUST) with detailed equations derivation and analysis.

<br/>

An accurate and reliable navigation system is essential for an autonomous vehicle to operate effectively and safely. It is difficult, if not impossible, to achieve precision self-localization for ground vehicles in various scenarios by using a single type or two types sensor. Thus, multisensor fusion navigation is of great significance for this challenging application. The vision based simultaneous localization and mapping system has attracted huge interests in both academic and industrial community due to its low-cost and ability to percept the environment around the vehicle.  

<br/>

I think it is valuable and exciting to build low cost and robust multisensor fusion navigation system for mobile robots to meet various requirements. Although abundant related works have been proposed and many problems have been solved well, challenges are existed such as building and reusing long-term map, V-SLAM (or VI-SLAM) in large scale area, indoor and outdoor continuous decimeter-level positioning and so on. 

